{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dnn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOda5JymwtRQP8ZZaJiriWL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"id":"rk0QaA4IK0G5","executionInfo":{"status":"ok","timestamp":1643350690191,"user_tz":-540,"elapsed":878,"user":{"displayName":"myelin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsdFK4eSI-MmpXs0HiaWPk3kxdZxZsOssXuZ2b6w=s64","userId":"09040177788199621057"}}},"outputs":[],"source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","import argparse\n","\n","import numpy as np\n","from numpy.random import default_rng\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader, Subset\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose\n","\n","\n","def initSeed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def repeatIndex(index, target_num):\n","    ite = target_num // len(index)\n","    index = np.tile(index, ite + 1)[:target_num]\n","    return index\n","\n","\n","class Trainer:\n","    def __init__(self, model, trainset, testset, cfg):\n","        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","        self.num_epochs = cfg['num_epochs']\n","        self.model = model.to(self.device)\n","        self.optimizer = SGD(self.model.parameters(), lr=cfg['lr'], momentum=0.9, nesterov=True, weight_decay=1e-4)\n","        batch_size = cfg['batch_size']\n","        self.trainloader = DataLoader(trainset, batch_size=batch_size, \n","                                shuffle=True, \n","                                num_workers=2, \n","                                pin_memory=True)\n","        self.testloader = DataLoader(testset, batch_size=batch_size,\n","                                shuffle=False,\n","                                num_workers=2,\n","                                pin_memory=True)\n","\n","\n","    def train(self):\n","        self.model.train()\n","        trainloss = 0\n","        for data in self.trainloader:\n","            inputs, labels = data\n","            inputs, labels = inputs.to(self.device), labels.to(self.device)\n","            self.optimizer.zero_grad()\n","            outputs = self.model(inputs)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss.backward()\n","            self.optimizer.step()\n","            trainloss += loss.item() * inputs.size()[0]\n","\n","        trainloss = trainloss / len(self.trainloader.dataset)\n","        return trainloss\n","\n","\n","    def test(self):\n","        self.model.eval()\n","        testacc = 0\n","        with torch.no_grad():\n","            for data in self.testloader:\n","                inputs, labels = data\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","                outputs = self.model(inputs)\n","                _, predicted = torch.max(outputs.data, 1)\n","                testacc += (predicted == labels).sum().item()\n","\n","        testacc = 100 * testacc / len(self.testloader.dataset)\n","        return testacc\n","\n","\n","    def run(self):\n","        for epoch in range(self.num_epochs):\n","            trainloss = self.train()\n","            testacc = self.test()\n","            print(f'epoch:{epoch+1}, trainloss:{trainloss:.3f}, testacc:{testacc:.1f}%')\n","\n","\n","def main(args):\n","    parser=argparse.ArgumentParser()\n","    parser.add_argument('--num_epochs', type=int, default=100)\n","    parser.add_argument('--batch_size', type=int, default=256)\n","    parser.add_argument('--lr', type=float, default=0.001)\n","    parser.add_argument('--momentum', type=float, default=0.9)\n","    parser.add_argument('--weight_decay', type=float, default=1e-4)\n","    parser.add_argument('--seed', type=int, default=42)\n","    parser.add_argument('--index', type=str, help='Use this option in \\'condensed\\' mode only')\n","    parser.add_argument('--mode', type=str, help='\\'full\\', \\'condensed\\', or \\'random\\'')\n","    parser.add_argument('--num_samples', type=int, default=20953, help='Use this option in \\'random\\' mode only')\n","    parser.add_argument('--repeat', action='store_true', help='Repeat the index to align the apparent number of samples with the all data set. If this option is flaged, the number of iterations is kept the same.')\n","    args = parser.parse_args(args=args)\n","\n","    initSeed(args.seed)\n","\n","    cfg = {\n","        'num_epochs': args.num_epochs,\n","        'batch_size': args.batch_size,\n","        'lr': args.lr,\n","        'momentum':args.momentum,\n","        'weight_decay': args.weight_decay\n","    }\n","    print('config:', cfg)\n","\n","    transform = Compose([\n","        ToTensor(),\n","        Normalize(mean=0.5, std=0.5)\n","    ])\n","    \n","    trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n","    testset = MNIST(root='./data', train=False, download=True, transform=transform)\n","    if args.mode == 'condensed':\n","        print('use condensed subset')\n","        if args.index is not None:\n","            index_condensed = np.loadtxt(args.index, dtype=int)\n","        else:\n","            raise ValueError\n","        if args.repeat:\n","            index_condensed = repeatIndex(index_condensed, len(trainset))\n","        trainset = Subset(trainset, index_condensed)\n","    elif args.mode == 'random':\n","        print('use random subset')\n","        rng = default_rng(args.seed)\n","        index_random = np.arange(len(trainset))\n","        rng.shuffle(index_random)\n","        index_random = index_random[:args.num_samples]\n","        if args.repeat:\n","            index_random = repeatIndex(index_random, len(trainset))\n","        trainset = Subset(trainset, index_random)\n","    else:\n","        print('use all dataset')\n","    \n","    print(f'num_samples:{len(trainset)}')\n","\n","    model = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(784, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 10)\n","    )\n","\n","    print('start training')\n","    trainer = Trainer(model, trainset, testset, cfg)\n","    trainer.run()\n","    print('\\ntrain finished!')"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/shinome551/condensedMNIST/main/index/condensedMNIST.txt\n","index_path = '/content/condensedMNIST.txt'"],"metadata":{"id":"59nzH9TOSxz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = [\n","    '--num_epochs',   '100',       ## If 50 is set to, the results are approximately same.\n","    '--batch_size',   '256',\n","    '--lr',           '0.001',\n","    '--momentum',     '0.9',\n","    '--weight_decay', '1e-4',\n","    '--seed',         '42',\n","    '--index',        index_path,  ## Use this option in 'condensed' mode only\n","    '--mode',         'condensed', ## 'full', 'condensed', or 'random'\n","    '--num_samples',  '20953',     ## Use this option in 'random' mode only\n","    '--repeat'                     ## Repeat the index to align the apparent number of samples with the all data set. If this option is flaged, the number of iterations is kept the same.\n","]\n","main(args)"],"metadata":{"id":"YgZLvHErK60H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uMbRhUeXLBR7"},"execution_count":null,"outputs":[]}]}